---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Import libraries

```{r}
library(reticulate)

torch      <- import("torch")
Variable   <- import("torch.autograd")$Variable
np         <- import("numpy")
optim      <- import("torch.optim") 
py         <- import_builtins()

data_util  <- py_run_file("data_util.py")
load_mnist <- data_util$load_mnist
```



```{r}
build_model <- function(input_dim, output_dim) {
    # We don't need the softmax layer here since CrossEntropyLoss already
    # uses it internally.
    model <- torch$nn$Sequential()
    model$add_module("linear",
                     torch$nn$Linear(input_dim, output_dim, bias = FALSE))
    return(model)
}

train <- function(model, loss, optimizer, x, y) {
    
    x = Variable(x, requires_grad = FALSE)
    y = Variable(y, requires_grad = FALSE)
    
    # reset gradient
    optimizer$zero_grad()
    
    # forward
    fx     <- model$forward(x)
    output <- loss$forward(fx, y)
    
    # backward
    output$backward()
    
    # update parameters
    optimizer$step()
    
    return(output$data$index(0L))
}

predict <- function(model, x) {
    xvar <-  Variable(x, requires_grad = FALSE)
    output = model$forward(xvar)
    # return(output$data$numpy()$argmax(axis = 1))
    return(np$argmax(output$data, axis = 1))
}
```

```{r}
# reproducible
torch$manual_seed(42L)

# load or download MNIST dataset
mnist <- load_mnist(onehot = FALSE)

trX = mnist[[1]]; teX = mnist[[2]]; trY = mnist[[3]]; teY = mnist[[4]]

trX <- torch$from_numpy(trX)$float()      # FloatTensor
teX <- torch$from_numpy(teX)$float()      # FloatTensor
trY <- torch$from_numpy(trY)$long()       # LongTensor
teY <- torch$from_numpy(teY)$long()       # LongTensor

# in Python was: n_examples, n_features = trX.size()
# using new R function torch_size()
n_examples <- torch_size(trX$size())[1]
n_features <- torch_size(trX$size())[2]

n_classes  <- 10L
model  <- build_model(n_features, n_classes)
loss   <- torch$nn$CrossEntropyLoss(size_average = TRUE)
optimizer  <- optim$SGD(model$parameters(), lr = 0.01, momentum = 0.9)
batch_size <- 100L


batching <- function(k) {
    k <- k - 1                             # index in Python start at [0]
    start <- as.integer(k * batch_size)
    end   <- as.integer((k + 1) * batch_size)
    
    cost  <- train(model, loss, optimizer,
                       trX$narrow(0L, start, end-start),
                       trY$narrow(0L, start, end-start))
    
    ccost <- ccost + cost$numpy()   # because we don't have yet `+` func
    predY <- predict(model, teX)
    return(list(predY = predY, ccost = ccost))
}
```


```{r}

for (i in seq(1, 50)) {
    ccost <- 0.0
    num_batches <- n_examples %/% batch_size
    # for (k in seq(1, num_batches)) {
    #     batch_li <- batching(k)
    # }
    batch_li <- lapply(seq(1, num_batches), batching)
    ccost <- batch_li[[600]]$ccost
    predY <- batch_li[[600]]$predY
    cat(sprintf("Epoch = %3d, cost = %f, acc = %.2f%% \n",
                i, ccost / num_batches, 
                100 * mean(predY$numpy() == teY$numpy())))
}
```



## sandbox


```{r}
# reproducible
torch$manual_seed(42L)

# load or download MNIST dataset
mnist <- load_mnist(onehot = FALSE)

trX = mnist[[1]]; teX = mnist[[2]]; trY = mnist[[3]]; teY = mnist[[4]]

trX <- torch$from_numpy(trX)$float()      # FloatTensor
teX <- torch$from_numpy(teX)$float()      # FloatTensor
trY <- torch$from_numpy(trY)$long()       # LongTensor
teY <- torch$from_numpy(teY)$long()       # LongTensor

# in Python was: n_examples, n_features = trX.size()
# using new R function torch_size()
n_examples <- torch_size(trX$size())[1]
n_features <- torch_size(trX$size())[2]

n_classes  <- 10L
model  <- build_model(n_features, n_classes)
loss   <- torch$nn$CrossEntropyLoss(size_average = TRUE)
optimizer  <- optim$SGD(model$parameters(), lr = 0.01, momentum = 0.9)
batch_size <- 100L

# modify function to allow ccost to accumulate
batching <- function(k) {
    k <- k - 1                             # index in Python start at [0]
    start <- as.integer(k * batch_size)
    end   <- as.integer((k + 1) * batch_size)
    
    cost  <- train(model, loss, optimizer,
                       trX$narrow(0L, start, end-start),
                       trY$narrow(0L, start, end-start))
    
    ccost <<- ccost + cost$numpy()   # because we don't have yet `+` func
    # predY <- predict(model, teX)
    # return(list(predY = predY, ccost = ccost))
    return(list(model = model, cost = ccost))
}


# for (i in seq(1, 50)) {
#     ccost <- 0.0
#     num_batches <- n_examples %/% batch_size
#     for (k in seq(1, num_batches)) {
#         batch_li <- batching(k)
#         ccost <- ccost + batch_li$cost
#         predY <- predict(batch_li$model, teX)
#     }
#     cat(sprintf("Epoch = %3d, cost = %f, acc = %.2f%% \n",
#                 i, ccost / num_batches, 
#                 100 * mean(predY$numpy() == teY$numpy())))
# }


for (i in seq(1, 50)) {
    ccost <- 0.0
    num_batches <- n_examples %/% batch_size
    batch_li <- lapply(seq(1, num_batches), batching)[[600]]
    # ccost <- ccost + batch_li$cost
    ccost <- batch_li$cost
    predY <- predict(batch_li$model, teX)
    cat(sprintf("Epoch = %3d, cost = %f, acc = %.2f%% \n",
                i, ccost / num_batches,
                100 * mean(predY$numpy() == teY$numpy())))
}

# Epoch =   1, cost = 0.547787, acc = 90.15% 
# Epoch =  50, cost = 0.261484, acc = 92.42% 
```

```{r}
batching <- function(k) {
  k <- k - 1                             # index in Python start at [0]
  start <- as.integer(k * batch_size)
  end   <- as.integer((k + 1) * batch_size)
  
  cost  <- train(model, loss, optimizer,
                       trX$narrow(0L, start, end-start),
                       trY$narrow(0L, start, end-start))

  ccost <- ccost + cost$numpy()   # because we don't have yet `+` func
  predY <- predict(model, teX)
  return(list(predY = predY, ccost = ccost))
}


for (i in seq(1, 50)) {
    ccost <- 0.0
    num_batches <- n_examples %/% batch_size
    for (k in seq(1, num_batches)) {
        batch_li <- batching(k)
    }
    ccost <- batch_li$ccost
    predY <- batch_li$predY
    cat(sprintf("Epoch = %3d, cost = %f, acc = %.2f%% \n",
                i, ccost / num_batches, 
                100 * mean(predY$numpy() == teY$numpy())))
}
```


```{r}
for (i in seq(1, 50)) {
    ccost <- 0.0
    num_batches <- n_examples %/% batch_size
    for (k in seq(1, num_batches)) {
        k <- k - 1                             # index in Python start at [0]
        start <- as.integer(k * batch_size)
        end   <- as.integer((k + 1) * batch_size)
        
        cost  <- train(model, loss, optimizer,
                             trX$narrow(0L, start, end-start),
                             trY$narrow(0L, start, end-start))

        ccost <- ccost + cost$numpy()   # because we don't have yet `+` func
        predY <- predict(model, teX)
        
    }
    cat(sprintf("Epoch = %3d, cost = %f, acc = %.2f%% \n",
                i, ccost / num_batches, 
                100 * mean(predY$numpy() == teY$numpy())))
}
```


```{r}
for (i in seq(1, 5)) {
    ccost <- 0.0
    num_batches <- n_examples %/% batch_size
    for (k in seq(1, num_batches)) {
        k <- k - 1                             # index in Python start at [0]
        start <- as.integer(k * batch_size)
        end   <- as.integer((k + 1) * batch_size)
        
        cost  <- train(model, loss, optimizer,
                             trX$narrow(0L, start, end-start),
                             trY$narrow(0L, start, end-start))

        ccost <- ccost + cost$numpy()   # because we don't have yet `+` func
        
        predY <- predict(model, teX)
        
        # xv <-  Variable(teX, requires_grad = FALSE)
        # output = model$forward(xv)
        # data1 <- output$data
        # predY <- np$argmax(data1, axis = 1)
    }
    cat(sprintf("Epoch = %3d, cost = %s, acc = %.2f%% \n",
                i, ccost / num_batches, 
                100 * mean(predY$numpy() == teY$numpy())))
}
```

